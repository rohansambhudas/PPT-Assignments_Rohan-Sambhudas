{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4a2d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1\n",
    "import configparser\n",
    "\n",
    "def display_hadoop_components():\n",
    "    # Read the Hadoop configuration file\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('hadoop.conf')  # Replace 'hadoop.conf' with the path to your Hadoop configuration file\n",
    "\n",
    "    # Display the core components of Hadoop\n",
    "    if 'core-site' in config:\n",
    "        core_components = config['core-site']\n",
    "        print(\"Core Components of Hadoop:\")\n",
    "        print(f\"NameNode: {core_components.get('fs.default.name')}\")\n",
    "        print(f\"SecondaryNameNode: {core_components.get('dfs.namenode.secondary.http-address')}\")\n",
    "        print(f\"DataNode: {core_components.get('dfs.datanode.http.address')}\")\n",
    "\n",
    "    if 'hdfs-site' in config:\n",
    "        hdfs_components = config['hdfs-site']\n",
    "        print(\"HDFS Components:\")\n",
    "        print(f\"Block Size: {hdfs_components.get('dfs.blocksize')}\")\n",
    "        print(f\"Replication Factor: {hdfs_components.get('dfs.replication')}\")\n",
    "\n",
    "    if 'yarn-site' in config:\n",
    "        yarn_components = config['yarn-site']\n",
    "        print(\"YARN Components:\")\n",
    "        print(f\"ResourceManager: {yarn_components.get('yarn.resourcemanager.hostname')}\")\n",
    "        print(f\"NodeManager: {yarn_components.get('yarn.nodemanager.hostname')}\")\n",
    "\n",
    "# Call the function to display Hadoop components\n",
    "display_hadoop_components()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8214d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2\n",
    "from pywebhdfs.webhdfs import PyWebHdfsClient\n",
    "\n",
    "def calculate_directory_size(hdfs_host, hdfs_port, hdfs_user, hdfs_directory):\n",
    "    # Connect to HDFS\n",
    "    hdfs = PyWebHdfsClient(host=hdfs_host, port=hdfs_port, user_name=hdfs_user)\n",
    "\n",
    "    # Get the file list in the directory\n",
    "    file_list = hdfs.list_dir(hdfs_directory)['FileStatuses']['FileStatus']\n",
    "\n",
    "    total_size = 0\n",
    "\n",
    "    # Iterate over the files in the directory\n",
    "    for file in file_list:\n",
    "        file_path = f\"{hdfs_directory}/{file['pathSuffix']}\"\n",
    "\n",
    "        # Get the file status to retrieve the file size\n",
    "        file_status = hdfs.get_file_status(file_path)['FileStatus']\n",
    "        file_size = file_status['length']\n",
    "\n",
    "        # Add the file size to the total\n",
    "        total_size += file_size\n",
    "\n",
    "    return total_size\n",
    "\n",
    "# Example usage\n",
    "hdfs_host = 'localhost'\n",
    "hdfs_port = 50070\n",
    "hdfs_user = 'hadoop'\n",
    "hdfs_directory = '/user/hadoop/data'\n",
    "\n",
    "total_size = calculate_directory_size(hdfs_host, hdfs_port, hdfs_user, hdfs_directory)\n",
    "print(f\"Total file size in {hdfs_directory}: {total_size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f468361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import heapq\n",
    "\n",
    "class TopNWords(MRJob):\n",
    "\n",
    "    def configure_args(self):\n",
    "        super(TopNWords, self).configure_args()\n",
    "        self.add_passthru_arg('--N', type=int, default=10, help='Number of top words to display')\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer=self.reducer),\n",
    "            MRStep(reducer=self.top_n_words)\n",
    "        ]\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        words = line.strip().split()\n",
    "        for word in words:\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    def combiner(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "    def top_n_words(self, _, word_count_pairs):\n",
    "        N = self.options.N\n",
    "        top_n = heapq.nlargest(N, word_count_pairs)\n",
    "        for count, word in top_n:\n",
    "            yield word, count\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    TopNWords.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237d4984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4\n",
    "import requests\n",
    "\n",
    "def check_namenode_health(namenode_host, namenode_port):\n",
    "    url = f\"http://{namenode_host}:{namenode_port}/jmx?qry=Hadoop:service=NameNode,name=NameNodeInfo\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        state = data['beans'][0]['State']\n",
    "        return state == 'active'\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def check_datanode_health(datanode_host, datanode_port):\n",
    "    url = f\"http://{datanode_host}:{datanode_port}/jmx?qry=Hadoop:service=DataNode,name=DataNodeInfo\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        state = data['beans'][0]['State']\n",
    "        return state == 'normal'\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    namenode_host = '<namenode_host>'\n",
    "    namenode_port = '<namenode_port>'\n",
    "    datanode_hosts = ['<datanode1_host>', '<datanode2_host>', ...]\n",
    "    datanode_port = '<datanode_port>'\n",
    "    \n",
    "    namenode_status = check_namenode_health(namenode_host, namenode_port)\n",
    "    print(f\"NameNode status: {'Healthy' if namenode_status else 'Unhealthy'}\")\n",
    "    \n",
    "    for datanode_host in datanode_hosts:\n",
    "        datanode_status = check_datanode_health(datanode_host, datanode_port)\n",
    "        print(f\"DataNode {datanode_host} status: {'Healthy' if datanode_status else 'Unhealthy'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a26b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5\n",
    "import pyarrow.hdfs as hdfs\n",
    "\n",
    "def list_hdfs_path(hdfs_host, hdfs_port, hdfs_path):\n",
    "    fs = hdfs.connect(host=hdfs_host, port=hdfs_port)\n",
    "    files = fs.ls(hdfs_path)\n",
    "    for file in files:\n",
    "        print(file)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    hdfs_host = '<hdfs_host>'\n",
    "    hdfs_port = <hdfs_port>\n",
    "    hdfs_path = '<hdfs_path>'\n",
    "    \n",
    "    list_hdfs_path(hdfs_host, hdfs_port, hdfs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5153852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 6\n",
    "import requests\n",
    "\n",
    "def analyze_data_node_storage(hadoop_host, hadoop_port):\n",
    "    # Fetch the DataNodes information from Hadoop's REST API\n",
    "    url = f\"http://{hadoop_host}:{hadoop_port}/jmx?qry=Hadoop:service=DataNode,name=FSDatasetState-*\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract storage capacities of DataNodes\n",
    "    datanodes = data['beans']\n",
    "    storage_capacities = {}\n",
    "    for datanode in datanodes:\n",
    "        storage_id = datanode['Storage'].split(\",\")[0].split(\"=\")[1]\n",
    "        capacity = datanode['Capacity']\n",
    "        storage_capacities[storage_id] = capacity\n",
    "\n",
    "    # Find the DataNode with the highest and lowest storage capacity\n",
    "    highest_capacity_node = max(storage_capacities, key=storage_capacities.get)\n",
    "    lowest_capacity_node = min(storage_capacities, key=storage_capacities.get)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"DataNode Storage Utilization Analysis:\")\n",
    "    print(\"======================================\")\n",
    "    print(f\"Highest Storage Capacity: {highest_capacity_node} - {storage_capacities[highest_capacity_node]}\")\n",
    "    print(f\"Lowest Storage Capacity: {lowest_capacity_node} - {storage_capacities[lowest_capacity_node]}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    hadoop_host = '<hadoop_host>'\n",
    "    hadoop_port = '<hadoop_port>'\n",
    "    \n",
    "    analyze_data_node_storage(hadoop_host, hadoop_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c994c087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 7\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def submit_hadoop_job(resource_manager_host, resource_manager_port, job_file, input_path, output_path):\n",
    "    # Submit the Hadoop job to the ResourceManager API\n",
    "    url = f\"http://{resource_manager_host}:{resource_manager_port}/ws/v1/cluster/apps\"\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {\n",
    "        'application-id': 'my-hadoop-job',\n",
    "        'application-name': 'My Hadoop Job',\n",
    "        'am-container-spec': {\n",
    "            'commands': {\n",
    "                'command': f\"hadoop jar {job_file} {input_path} {output_path}\"\n",
    "            },\n",
    "            'local-resources': {\n",
    "                'entry': [\n",
    "                    {\n",
    "                        'key': 'hadoop-mapreduce-examples.jar',\n",
    "                        'value': {\n",
    "                            'resource': 'file:/path/to/hadoop-mapreduce-examples.jar',\n",
    "                            'type': 'FILE'\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    if response.status_code == 202:\n",
    "        print(\"Hadoop job submitted successfully.\")\n",
    "    else:\n",
    "        print(\"Failed to submit Hadoop job.\")\n",
    "\n",
    "    # Monitor the progress of the Hadoop job\n",
    "    application_id = response.json()['application-id']\n",
    "    while True:\n",
    "        url = f\"http://{resource_manager_host}:{resource_manager_port}/ws/v1/cluster/apps/{application_id}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()['app']\n",
    "            state = data['state']\n",
    "            if state == 'FINISHED':\n",
    "                print(\"Hadoop job finished successfully.\")\n",
    "                break\n",
    "            elif state == 'FAILED':\n",
    "                print(\"Hadoop job failed.\")\n",
    "                break\n",
    "            else:\n",
    "                progress = data['progress']\n",
    "                print(f\"Hadoop job progress: {progress}\")\n",
    "        else:\n",
    "            print(\"Failed to retrieve Hadoop job status.\")\n",
    "            break\n",
    "\n",
    "        time.sleep(5)  # Wait for 5 seconds before checking again\n",
    "\n",
    "    # Retrieve the final output of the Hadoop job\n",
    "    if state == 'FINISHED':\n",
    "        url = f\"http://{resource_manager_host}:{resource_manager_port}/ws/v1/cluster/apps/{application_id}/attempts/1/containers\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            container_id = response.json()['containers']['container'][0]['containerId']\n",
    "            url = f\"http://{resource_manager_host}:{resource_manager_port}/ws/v1/node/containerlogs/{container_id}/stdout\"\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                output = response.text\n",
    "                print(\"Final output:\")\n",
    "                print(output)\n",
    "            else:\n",
    "                print(\"Failed to retrieve final output.\")\n",
    "        else:\n",
    "            print(\"Failed to retrieve container details.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    resource_manager_host = '<resource_manager_host>'\n",
    "    resource_manager_port = '<resource_manager_port>'\n",
    "    job_file = '<path_to_hadoop_job_jar_file>'\n",
    "    input_path = '<input_path>'\n",
    "    output_path = '<output_path>'\n",
    "\n",
    "    submit_hadoop_job(resource_manager_host, resource_manager_port, job_file, input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3028cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 8\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def submit_hadoop_job(resource_manager_host, resource_manager_port, job_file, input_path, output_path, num_containers, memory_mb, vcores):\n",
    "    # Submit the Hadoop job to the ResourceManager API\n",
    "    url = f\"http://{resource_manager_host}:{resource_manager_port}/ws/v1/cluster/apps\"\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {\n",
    "        'application-id': 'my-hadoop-job',\n",
    "        'application-name': 'My Hadoop Job',\n",
    "        'am-container-spec': {\n",
    "            'commands': {\n",
    "                'command': f\"hadoop jar {job_file} {input_path} {output_path}\"\n",
    "            },\n",
    "            'local-resources': {\n",
    "                'entry': [\n",
    "                    {\n",
    "                        'key': 'hadoop-mapreduce-examples.jar',\n",
    "                        'value': {\n",
    "                            'resource': 'file:/path/to/hadoop-mapreduce-examples.jar',\n",
    "                            'type': 'FILE'\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            'resources': {\n",
    "                'memory': memory_mb,\n",
    "                'vcores': vcores\n",
    "            },\n",
    "            'instances': num_containers\n",
    "        }\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    if response.status_code == 202:\n",
    "        print(\"Hadoop job submitted successfully.\")\n",
    "    else:\n",
    "        print(\"Failed to submit Hadoop job.\")\n",
    "\n",
    "    # Monitor the progress of the Hadoop job\n",
    "    application_id = response.json()['application-id']\n",
    "    while True:\n",
    "        url = f\"http://{resource_manager_host}:{resource_manager_port}/ws/v1/cluster/apps/{application_id}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()['app']\n",
    "            state = data['state']\n",
    "            if state == 'FINISHED':\n",
    "                print(\"Hadoop job finished successfully.\")\n",
    "                break\n",
    "            elif state == 'FAILED':\n",
    "                print(\"Hadoop job failed.\")\n",
    "                break\n",
    "            else:\n",
    "                progress = data['progress']\n",
    "                print(f\"Hadoop job progress: {progress}\")\n",
    "\n",
    "                # Track resource usage during job execution\n",
    "                tracking_url = data['tracking-ui']\n",
    "                url = f\"{tracking_url}/ws/v1/mapreduce/jobs/{application_id}/counters\"\n",
    "                response = requests.get(url)\n",
    "                if response.status_code == 200:\n",
    "                    counters = response.json()\n",
    "                    map_progress = counters['jobCounters']['counterGroup'][0]['counter'][0]['value']\n",
    "                    reduce_progress = counters['jobCounters']['counterGroup'][1]['counter'][0]['value']\n",
    "                    print(f\"Map progress: {map_progress}\")\n",
    "                    print(f\"Reduce progress: {reduce_progress}\")\n",
    "                else:\n",
    "                    print(\"Failed to retrieve job counters.\")\n",
    "\n",
    "        else:\n",
    "            print(\"Failed to retrieve Hadoop job status.\")\n",
    "            break\n",
    "\n",
    "        time.sleep(5)  # Wait for 5 seconds before checking again\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    resource_manager_host = '<resource_manager_host>'\n",
    "    resource_manager_port = '<resource_manager_port>'\n",
    "    job_file = '<path_to_hadoop_job_jar_file>'\n",
    "    input_path = '<input_path>'\n",
    "    output_path = '<output_path>'\n",
    "    num_containers = 2  # Number of containers (task instances)\n",
    "    memory_mb = 2048  # Memory per container in MB\n",
    "    vcores = 2  # Number of virtual cores per container\n",
    "\n",
    "    submit_hadoop_job(resource_manager_host, resource_manager_port, job_file, input_path, output_path, num_containers, memory_mb, vcores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cdafc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 9\n",
    "import time\n",
    "import subprocess\n",
    "\n",
    "def run_mapreduce_job(input_split_size):\n",
    "    # Set the input split size as a parameter for the MapReduce job\n",
    "    cmd = f\"hadoop jar <path_to_mapreduce_jar_file> <mapreduce_input_path> <mapreduce_output_path> -D mapreduce.input.fileinputformat.split.maxsize={input_split_size}\"\n",
    "    subprocess.call(cmd, shell=True)\n",
    "\n",
    "def compare_mapreduce_performance(input_split_sizes):\n",
    "    for split_size in input_split_sizes:\n",
    "        start_time = time.time()\n",
    "        run_mapreduce_job(split_size)\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"Input Split Size: {split_size} - Execution Time: {execution_time} seconds\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_split_sizes = [64 * 1024 * 1024, 128 * 1024 * 1024, 256 * 1024 * 1024]  # Example input split sizes in bytes\n",
    "    compare_mapreduce_performance(input_split_sizes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
